{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "app = \"/app\"\n",
    "if app not in sys.path:\n",
    "    sys.path.append(app)\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/emboj_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_dir=\"/data/emboj_abstracts/\",\n",
    "    data_files={'train': 'train/examples.txt', 'test': 'test/examples.txt'},\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"max length: {tokenizer.max_model_input_sizes[MODEL_NAME]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenization(examples):\n",
    "    return tokenizer(\n",
    "    examples[\"text\"],\n",
    "        max_length=64,  # tokenizer.max_model_input_sizes[MODEL_NAME],\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=False,\n",
    "    )\n",
    "tokenized = dataset.map(tokenization, batched=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenized.remove_columns([\"text\"])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    # RobertaForMaskedLM,\n",
    "    AutoConfig\n",
    ")\n",
    "from src.models.modeling_dendroberta import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_hidden_layers=8\n",
    "# config.intermediate_size=128\n",
    "config.max_position_embeddings=66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "(\n",
    "    torch.torch.backends.mps.is_built(),\n",
    "    torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 5\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def log(self, loss, epoch, writer):\n",
    "    # write to tensorboard\n",
    "    writer.add_scalar(\"Loss/train\", loss.data, epoch)\n",
    "    # loss on Trainset (we are lazy and don't use a separate validation set)\n",
    "    with torch.no_grad():\n",
    "        outputs, states = self.model(self.X_test)\n",
    "        valid_loss = self.criterion(outputs, self.y_test)\n",
    "    writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "writer = SummaryWriter(log_dir=\"/runs\")\n",
    "eval_steps = 100\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train(True)\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        step += 1\n",
    "\n",
    "        if step % eval_steps == 0:\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            losses = []\n",
    "            for batch in tqdm(eval_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    eval_outputs = model(**batch)\n",
    "                eval_loss = eval_outputs.loss\n",
    "                if eval_loss is not None:\n",
    "                    losses.append(accelerator.gather(eval_loss.repeat(batch_size)))\n",
    "                else:\n",
    "                    losses.append(torch.tensor(0))\n",
    "        \n",
    "            losses = torch.cat(losses)\n",
    "            losses = losses[: len(eval_dataset)]\n",
    "            avg_loss = torch.mean(losses)\n",
    "            writer.add_scalar(\"Loss/eval\", avg_loss, step)\n",
    "            try:\n",
    "                perplexity = math.exp(avg_loss)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "            writer.add_scalar(\"Perplexity/eval\", perplexity, step)\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        # repo.push_to_hub(\n",
    "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
