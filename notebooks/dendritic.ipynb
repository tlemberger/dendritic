{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4VG8OIMM0b8"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "APP = \"/app\" if Path(\"/app\").exists() else \"../app\"\n",
    "RUNS = \"/runs\" if Path(\"/runs\").exists() else \"../runs\"\n",
    "(APP, RUNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "app = APP\n",
    "if app not in sys.path:\n",
    "    sys.path.append(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k2okHqDHDSn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch import optim\n",
    "from torch.nn import Parameter, GELU, Tanh, Sigmoid, Linear, Conv2d\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi6hOrxfM3Wz"
   },
   "source": [
    "# A 'dendritic' clustering layer\n",
    "\n",
    "Inspired by Larkum ME, 2022, \"Are Dendrites Conceptually Useful, Neuroscience https://doi.org/10.1016/j.neuroscience.2022.03.008\n",
    "\n",
    "A 'dendritic' fully connected layer extends the classical fully connected `Linear` layer. It usess a convolution filter `conv_filter` to aggregate the activity of neighbouring synapses. The filter is moved along the sequence of synapses with the indicated `stride`. Note that this is a **fixed filter** -- it is NOT a learnable convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vDa0xsGMtdp"
   },
   "source": [
    "# Toy example with simple classification task by a 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3t_HJBCM-Wz"
   },
   "source": [
    "We create a dataset using sklean `make_moons` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 50\n",
    "N_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JThjwRq6HO4s"
   },
   "outputs": [],
   "source": [
    "# X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_classes=N_CLASSES,\n",
    "    n_features=N_FEATURES, n_informative=3, n_redundant=10, n_repeated=0,\n",
    "    random_state=42, n_clusters_per_class=1,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca reduce to 3 dimensions for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYaNFYB_NDhl"
   },
   "source": [
    "This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "zkl58MGcHfnh",
    "outputId": "82cead5c-997f-4214-966d-d12bd6b28bcb"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2],\n",
    "    color=y.astype(str),\n",
    "    labels={'color': 'Class'},\n",
    "    width=500, height=500,\n",
    "    title='2D Classification Dataset Created by classification'\n",
    ").update_traces(marker=dict(size=2)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QvoFq1jNGI3"
   },
   "source": [
    "We convert this into a torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zQgcCphHepz"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSwwMsqZHgoN"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdQ5Eg8DQe69"
   },
   "source": [
    "A simple training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjTBlOoTHmLZ",
    "outputId": "0e129c9f-67c5-437b-f213-f1b7a5d71468"
   },
   "outputs": [],
   "source": [
    "\n",
    "# this was an attempt to include a network-level constraint \n",
    "# to clamp the state to a given upstate max value\n",
    "\n",
    "# def loss_fn(outputs, states, labels):\n",
    "#     alpha = 0.5  # state regularization coefficient\n",
    "#     up_state = 0  # upper bound for state regularization\n",
    "#     # include a constraint on the state to encourage clamping it\n",
    "#     # to up_state\n",
    "#     if states is not None:\n",
    "#         states = [torch.relu(s - up_state).mean() for s in states]\n",
    "#         state_regul = sum(states) / len(states)\n",
    "#     else:\n",
    "#         state_regul = torch.tensor(0)\n",
    "#     loss = criterion(outputs, labels) + alpha * state_regul\n",
    "#     return loss, state_regul\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, testdata):\n",
    "      self.model = model\n",
    "      self.criterion = nn.CrossEntropyLoss()\n",
    "      self.train_loader = train_loader\n",
    "      self.X_test = testdata[0]\n",
    "      self.y_test = testdata[1]\n",
    "\n",
    "    def train(self, epochs: int, lr: float, run_name:str):\n",
    "        writer = SummaryWriter(run_name)  # open new writer --> /runs\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            for inputs, labels in self.train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)  # loss_fn(outputs, states, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.log(loss, epoch, writer)\n",
    "        print(f\"Done at epoch {epoch}, Loss: {loss.item()}\")\n",
    "        self.accuracy()\n",
    "        writer.close()\n",
    "\n",
    "    def log(self, loss, epoch, writer):\n",
    "        # write to tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", loss.data, epoch)\n",
    "        # loss on Trainset (we are lazy and don't use a separate validation set)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(self.X_test)\n",
    "            valid_loss = self.criterion(outputs, self.y_test)\n",
    "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "\n",
    "    def accuracy(self):\n",
    "        # checking accuracy quickly\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(self.X_test)\n",
    "            predicted = outputs.argmax(1)\n",
    "            accuracy = (predicted == self.y_test).sum().item() / len(self.y_test)\n",
    "        print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqxwDbSENRkl"
   },
   "source": [
    "Define the model as a 2-layer MLP. The first layer is the 'dendritic' layer. It usess a convolution filter to aggregate the activity of neighbouring synapses. The filter is moved along the sequence of synapses with the indicated stride. This is a **fixed filter** -- it is NOT a learnable convolution. The second layer is a normal `Linear` module as classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN = 3072  # as in RoBeRtA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HOEKGvDHjjJ"
   },
   "outputs": [],
   "source": [
    "class dMLP(nn.Module):\n",
    "    \"\"\"'dendritic' MLP with 2 hidden layers, the first classic to expand,\n",
    "    the next dendritic to integrate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stride, conv_filter, **kwargs):\n",
    "        super(dMLP, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.conv_filter = conv_filter\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(N_FEATURES, HIDDEN)\n",
    "        # self.fc1 = dd.DendriticFullyConnected(N_FEATURES, HIDDEN, conv_filter=self.conv_filter, stride=self.stride, clustering_frac=0.2, **kwargs)\n",
    "        self.fc2 = dd.DendriticFullyConnected(HIDDEN, N_CLASSES, conv_filter=self.conv_filter, stride=self.stride, clustering_frac=0.1, **kwargs)\n",
    "        # self.fc2 = nn.Linear(HIDDEN, N_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act_fn(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baZZqPR6NmZU"
   },
   "source": [
    "For comparison, a similar classical MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLLvRNcbHlDZ"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Classical MLP with 2 layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_FEATURES, HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, N_CLASSES)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act_fn(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from src.models.dd import Hill, MyRELU\n",
    "\n",
    "# normal \n",
    "mlp = MLP()\n",
    "model = (mlp,)\n",
    "run_name = (f\"{RUNS}/MLP-{datetime.now().isoformat().replace(':','-')}\",)\n",
    "\n",
    "# dendritic MLP\n",
    "stride=4\n",
    "kernel_size=10\n",
    "conv_filter =  torch.tensor([[[1/kernel_size] * kernel_size]])    # torch.tensor([[[1/kernel_size]* kernel_size]])  # torch.tensor([[[0.001, 0.01, 0.2, 0.5, 0.2, 0.01, 0.001]]])  # torch.tensor([[[1/kernel_size]* kernel_size]])\n",
    "dmlp = dMLP(\n",
    "    stride=stride,\n",
    "    conv_filter=conv_filter,\n",
    "    cluster_act_fn=Hill(2, 0.5),\n",
    "    bias=True\n",
    ")\n",
    "model += (dmlp,)\n",
    "run_name += (f\"{RUNS}/dMLP-{datetime.now().isoformat().replace(':','-')}\",)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMDA antagonist simulation\n",
    "# params = dict(p for p in dmlp.named_parameters())\n",
    "# params['fc1.nmda.weight'].requires_grad_(False)\n",
    "# params['fc1.nmda.bias'].requires_grad_(False)\n",
    "# params['fc1.non_nmda.weight'].requires_grad_(False)\n",
    "# params['fc1.non_nmda.bias'].requires_grad_(False)\n",
    "# same for fc2\n",
    "# params['fc2.weight'].requires_grad_(False)\n",
    "# params['fc2.bias'].requires_grad_(False)\n",
    "# params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "for run, m in zip(run_name, model):\n",
    "    print(f\"training {run}\")\n",
    "    print(m)\n",
    "    trainer = Trainer(m, train_loader, (X_test, y_test))\n",
    "    trainer.train(epochs=1000, lr=0.001, run_name=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9o_iZ9wHr7L"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch tensorboard\n",
    "%tensorboard --logdir ../runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSEAtmI9QnOZ"
   },
   "source": [
    "Visualization of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model[1].named_modules(): print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[1].fc1.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "SnSdl3ouC1Dp",
    "outputId": "11598bf0-c520-4fe0-d5ad-65ae0555cfe0"
   },
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "    dict(model[1].named_modules())['fc2.non_nmda'].weight.data,\n",
    "    width=1000, aspect='auto',\n",
    ").update_layout(coloraxis_showscale=False).show()\n",
    "print(dict(model[1].named_modules())['fc2.non_nmda'].weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "I91BytxbmoFH",
    "outputId": "e0eb72cd-6631-4211-f337-94a5951a3b99"
   },
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "    dict(model[1].named_modules())['fc2.nmda'].weight.data,\n",
    "    width=1000, aspect='auto',\n",
    ").update_layout(coloraxis_showscale=False).show()\n",
    "print(dict(model[1].named_modules())['fc2.nmda'].weight.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
